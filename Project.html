<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_e6q17l41uhvy-4>li{counter-increment:lst-ctn-kix_e6q17l41uhvy-4}ul.lst-kix_9u12hsoielar-4{list-style-type:none}ul.lst-kix_9u12hsoielar-3{list-style-type:none}.lst-kix_e6q17l41uhvy-0>li:before{content:"" counter(lst-ctn-kix_e6q17l41uhvy-0,decimal) ". "}.lst-kix_e6q17l41uhvy-1>li:before{content:"" counter(lst-ctn-kix_e6q17l41uhvy-1,lower-latin) ". "}ul.lst-kix_9u12hsoielar-6{list-style-type:none}ul.lst-kix_9u12hsoielar-5{list-style-type:none}ul.lst-kix_9u12hsoielar-0{list-style-type:none}.lst-kix_e6q17l41uhvy-2>li:before{content:"" counter(lst-ctn-kix_e6q17l41uhvy-2,lower-roman) ". "}ul.lst-kix_9u12hsoielar-2{list-style-type:none}ul.lst-kix_9u12hsoielar-1{list-style-type:none}.lst-kix_e6q17l41uhvy-5>li{counter-increment:lst-ctn-kix_e6q17l41uhvy-5}.lst-kix_e6q17l41uhvy-3>li{counter-increment:lst-ctn-kix_e6q17l41uhvy-3}.lst-kix_9u12hsoielar-7>li:before{content:"\0025cb  "}.lst-kix_9u12hsoielar-8>li:before{content:"\0025a0  "}.lst-kix_9u12hsoielar-5>li:before{content:"\0025a0  "}ol.lst-kix_e6q17l41uhvy-8{list-style-type:none}.lst-kix_e6q17l41uhvy-6>li{counter-increment:lst-ctn-kix_e6q17l41uhvy-6}ol.lst-kix_e6q17l41uhvy-7{list-style-type:none}ol.lst-kix_e6q17l41uhvy-6{list-style-type:none}ol.lst-kix_e6q17l41uhvy-5{list-style-type:none}ol.lst-kix_e6q17l41uhvy-8.start{counter-reset:lst-ctn-kix_e6q17l41uhvy-8 0}.lst-kix_9u12hsoielar-3>li:before{content:"\0025cf  "}.lst-kix_9u12hsoielar-4>li:before{content:"\0025cb  "}ol.lst-kix_e6q17l41uhvy-4{list-style-type:none}ol.lst-kix_e6q17l41uhvy-3{list-style-type:none}ol.lst-kix_e6q17l41uhvy-2{list-style-type:none}ul.lst-kix_9u12hsoielar-8{list-style-type:none}ol.lst-kix_e6q17l41uhvy-1{list-style-type:none}.lst-kix_9u12hsoielar-2>li:before{content:"\0025a0  "}ul.lst-kix_9u12hsoielar-7{list-style-type:none}ol.lst-kix_e6q17l41uhvy-0{list-style-type:none}.lst-kix_9u12hsoielar-6>li:before{content:"\0025cf  "}ol.lst-kix_e6q17l41uhvy-5.start{counter-reset:lst-ctn-kix_e6q17l41uhvy-5 0}ul.lst-kix_mlc9zhu4g1nu-6{list-style-type:none}.lst-kix_p1af4nr2px9q-1>li:before{content:"\0025cb  "}ul.lst-kix_mlc9zhu4g1nu-7{list-style-type:none}ul.lst-kix_mlc9zhu4g1nu-8{list-style-type:none}.lst-kix_p1af4nr2px9q-3>li:before{content:"\0025cf  "}.lst-kix_p1af4nr2px9q-2>li:before{content:"\0025a0  "}.lst-kix_p1af4nr2px9q-5>li:before{content:"\0025a0  "}ol.lst-kix_e6q17l41uhvy-2.start{counter-reset:lst-ctn-kix_e6q17l41uhvy-2 0}ul.lst-kix_mlc9zhu4g1nu-0{list-style-type:none}ul.lst-kix_mlc9zhu4g1nu-1{list-style-type:none}ul.lst-kix_mlc9zhu4g1nu-2{list-style-type:none}ul.lst-kix_mlc9zhu4g1nu-3{list-style-type:none}ul.lst-kix_mlc9zhu4g1nu-4{list-style-type:none}.lst-kix_p1af4nr2px9q-4>li:before{content:"\0025cb  "}ul.lst-kix_mlc9zhu4g1nu-5{list-style-type:none}ul.lst-kix_p1af4nr2px9q-2{list-style-type:none}ul.lst-kix_p1af4nr2px9q-1{list-style-type:none}ul.lst-kix_p1af4nr2px9q-4{list-style-type:none}ul.lst-kix_p1af4nr2px9q-3{list-style-type:none}ul.lst-kix_p1af4nr2px9q-6{list-style-type:none}ul.lst-kix_p1af4nr2px9q-5{list-style-type:none}ul.lst-kix_p1af4nr2px9q-8{list-style-type:none}.lst-kix_e6q17l41uhvy-2>li{counter-increment:lst-ctn-kix_e6q17l41uhvy-2}ul.lst-kix_p1af4nr2px9q-7{list-style-type:none}.lst-kix_p1af4nr2px9q-0>li:before{content:"\0025cf  "}.lst-kix_e6q17l41uhvy-8>li{counter-increment:lst-ctn-kix_e6q17l41uhvy-8}.lst-kix_e6q17l41uhvy-8>li:before{content:"" counter(lst-ctn-kix_e6q17l41uhvy-8,lower-roman) ". "}.lst-kix_e6q17l41uhvy-6>li:before{content:"" counter(lst-ctn-kix_e6q17l41uhvy-6,decimal) ". "}.lst-kix_e6q17l41uhvy-5>li:before{content:"" counter(lst-ctn-kix_e6q17l41uhvy-5,lower-roman) ". "}.lst-kix_e6q17l41uhvy-4>li:before{content:"" counter(lst-ctn-kix_e6q17l41uhvy-4,lower-latin) ". "}ul.lst-kix_p1af4nr2px9q-0{list-style-type:none}.lst-kix_e6q17l41uhvy-3>li:before{content:"" counter(lst-ctn-kix_e6q17l41uhvy-3,decimal) ". "}ol.lst-kix_e6q17l41uhvy-3.start{counter-reset:lst-ctn-kix_e6q17l41uhvy-3 0}.lst-kix_e6q17l41uhvy-7>li:before{content:"" counter(lst-ctn-kix_e6q17l41uhvy-7,lower-latin) ". "}ul.lst-kix_7jfpdnsyzbnl-4{list-style-type:none}ul.lst-kix_7jfpdnsyzbnl-5{list-style-type:none}ul.lst-kix_7jfpdnsyzbnl-2{list-style-type:none}ul.lst-kix_7jfpdnsyzbnl-3{list-style-type:none}ul.lst-kix_7jfpdnsyzbnl-0{list-style-type:none}ul.lst-kix_7jfpdnsyzbnl-1{list-style-type:none}ol.lst-kix_e6q17l41uhvy-4.start{counter-reset:lst-ctn-kix_e6q17l41uhvy-4 0}ul.lst-kix_7jfpdnsyzbnl-8{list-style-type:none}ol.lst-kix_e6q17l41uhvy-7.start{counter-reset:lst-ctn-kix_e6q17l41uhvy-7 0}ul.lst-kix_7jfpdnsyzbnl-6{list-style-type:none}ul.lst-kix_7jfpdnsyzbnl-7{list-style-type:none}ol.lst-kix_e6q17l41uhvy-1.start{counter-reset:lst-ctn-kix_e6q17l41uhvy-1 0}.lst-kix_7jfpdnsyzbnl-0>li:before{content:"\0025cf  "}.lst-kix_mlc9zhu4g1nu-0>li:before{content:"\0025cf  "}.lst-kix_mlc9zhu4g1nu-2>li:before{content:"\0025a0  "}.lst-kix_mlc9zhu4g1nu-1>li:before{content:"\0025cb  "}.lst-kix_mlc9zhu4g1nu-6>li:before{content:"\0025cf  "}.lst-kix_mlc9zhu4g1nu-4>li:before{content:"\0025cb  "}.lst-kix_mlc9zhu4g1nu-8>li:before{content:"\0025a0  "}.lst-kix_p1af4nr2px9q-7>li:before{content:"\0025cb  "}.lst-kix_mlc9zhu4g1nu-3>li:before{content:"\0025cf  "}.lst-kix_mlc9zhu4g1nu-7>li:before{content:"\0025cb  "}.lst-kix_p1af4nr2px9q-6>li:before{content:"\0025cf  "}.lst-kix_mlc9zhu4g1nu-5>li:before{content:"\0025a0  "}.lst-kix_p1af4nr2px9q-8>li:before{content:"\0025a0  "}.lst-kix_e6q17l41uhvy-0>li{counter-increment:lst-ctn-kix_e6q17l41uhvy-0}.lst-kix_e6q17l41uhvy-7>li{counter-increment:lst-ctn-kix_e6q17l41uhvy-7}.lst-kix_e6q17l41uhvy-1>li{counter-increment:lst-ctn-kix_e6q17l41uhvy-1}.lst-kix_9u12hsoielar-0>li:before{content:"\0025cf  "}.lst-kix_9u12hsoielar-1>li:before{content:"\0025cb  "}ol.lst-kix_e6q17l41uhvy-6.start{counter-reset:lst-ctn-kix_e6q17l41uhvy-6 0}.lst-kix_7jfpdnsyzbnl-3>li:before{content:"\0025cf  "}.lst-kix_7jfpdnsyzbnl-1>li:before{content:"\0025cb  "}.lst-kix_7jfpdnsyzbnl-5>li:before{content:"\0025a0  "}.lst-kix_7jfpdnsyzbnl-2>li:before{content:"\0025a0  "}.lst-kix_7jfpdnsyzbnl-6>li:before{content:"\0025cf  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_7jfpdnsyzbnl-4>li:before{content:"\0025cb  "}ol.lst-kix_e6q17l41uhvy-0.start{counter-reset:lst-ctn-kix_e6q17l41uhvy-0 0}.lst-kix_7jfpdnsyzbnl-7>li:before{content:"\0025cb  "}.lst-kix_7jfpdnsyzbnl-8>li:before{content:"\0025a0  "}ol{margin:0;padding:0}table td,table th{padding:0}.c33{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:80.2pt;border-top-color:#000000;border-bottom-style:solid}.c8{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:141.8pt;border-top-color:#000000;border-bottom-style:solid}.c41{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:183pt;border-top-color:#000000;border-bottom-style:solid}.c19{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:70.5pt;border-top-color:#000000;border-bottom-style:solid}.c30{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75.8pt;border-top-color:#000000;border-bottom-style:solid}.c6{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:72.8pt;border-top-color:#000000;border-bottom-style:solid}.c12{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:135.8pt;border-top-color:#000000;border-bottom-style:solid}.c23{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:78pt;border-top-color:#000000;border-bottom-style:solid}.c14{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:91.5pt;border-top-color:#000000;border-bottom-style:solid}.c17{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:27.8pt;border-top-color:#000000;border-bottom-style:solid}.c9{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c25{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:76.5pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:156.8pt;border-top-color:#000000;border-bottom-style:solid}.c7{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:65.2pt;border-top-color:#000000;border-bottom-style:solid}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c37{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c18{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c34{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c11{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c36{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:24pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c21{color:#212529;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c35{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c42{color:#000000;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c27{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c32{border-spacing:0;border-collapse:collapse;margin-right:auto}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c22{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c4{font-size:14pt;font-weight:700}.c24{margin-left:36pt;padding-left:0pt}.c49{font-weight:400;font-size:18pt}.c31{color:inherit;text-decoration:inherit}.c51{font-weight:400;font-size:16pt}.c45{font-weight:400;font-size:26pt}.c28{padding:0;margin:0}.c38{height:148.2pt}.c39{height:20pt}.c44{font-size:24pt}.c5{height:11pt}.c43{height:132pt}.c48{height:108.9pt}.c50{vertical-align:super}.c29{color:#212529}.c3{background-color:#ffffff}.c13{font-size:12pt}.c47{height:129.8pt}.c16{font-weight:700}.c15{height:0pt}.c46{height:167.2pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c3 c22"><p class="c34 title" id="h.9ncep4eshfxm"><span class="c42 c45">Heart Disease Prediction</span></p><p class="c34 title" id="h.r3fnw08nf6gd"><span class="c36">Machine Learning</span></p><p class="c34 title" id="h.yeqja5eooj4z"><span class="c44">Capstone Project</span><span class="c42 c45">&nbsp;</span></p><p class="c34 title" id="h.e4oifclnbwf9"><span class="c42 c49">Henriette Piringer</span></p><h1 class="c18 c39" id="h.co7w9ope64s"><span class="c40"></span></h1><h1 class="c18" id="h.a6fzdbwfdrmb"><span class="c40">Goals of the project</span></h1><p class="c1"><span class="c2">The goal of this project is to demonstrate an end-to-end Machine Learning project through a practical problem. The main steps are:</span></p><ol class="c28 lst-kix_e6q17l41uhvy-0 start" start="1"><li class="c1 c24 li-bullet-0"><span class="c2">Recognize the problem.</span></li><li class="c1 c24 li-bullet-0"><span class="c2">Acquire the data.</span></li><li class="c1 c24 li-bullet-0"><span class="c2">Explore the data and gain insight with visualization.</span></li><li class="c1 c24 li-bullet-0"><span class="c2">Prepare the data for Machine Learning algorithms.</span></li><li class="c1 c24 li-bullet-0"><span class="c2">Train several models and choose the best ones.</span></li><li class="c1 c24 li-bullet-0"><span class="c2">Fine tune the models.</span></li><li class="c1 c24 li-bullet-0"><span class="c2">Present and interpret the solution.</span></li><li class="c1 c24 li-bullet-0"><span class="c13">Create a web </span><span class="c13">application</span><span class="c13">&nbsp;and user </span><span class="c13">interface</span><span class="c2">, then deploy, monitor and maintain the system.</span></li></ol><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">For this project, I chose to predict a health condition: ischemic heart disease.</span></p><p class="c1"><span class="c2">One of the leading causes of death in the United States and around the world is heart disease and heart attacks (CDC, NCHS Data Brief, 2021). According to the CDC, in 2020, about 670 thousand people died from heart disease, and the numbers are growing every year. There are several well documented risk factors that doctors use to diagnose ischemic heart disease: cholesterol levels, high blood pressure, age, obesity, diabetes, angina, a type of chest pain, or ECG abnormalities. In addition to these risk factors, there are several more lifestyle related factors that are worth considering like physical activity, sleep length and quality, medications, vitamins, fruit, vegetables, meat consumption, smoking and alcohol use. </span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">Taking advantage of the large amount of data available today and Machine Learning algorithms, it makes sense to use this technology in diagnostics. Prevention and early diagnosis can be crucial in such a deadly disease, especially since in many cases there are no physical symptoms before the first heart attack. The use of an effective ML model could aid doctors to identify high risk patients and start intervention early either with treatment or lifestyle changes. &nbsp;</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13">The National Heart Foundation of Australia (N.H.F.O.) conducts a comprehensive health survey regularly to </span><span class="c13">study the collected data and </span><span class="c2">evaluate risk factors and additional lifestyle choices. I used the 1980 dataset: &ldquo;Risk Factor Prevalence Study, 1980&rdquo;.</span></p><p class="c1"><span class="c2">From the 169 columns of the dataset, I used 35 as features to predict a heart attack in a classification problem. </span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13">The project was conducted in Jupyter Notebook with Python as it provides a nice work environment and I was already familiar with it. For the web application, FastAPI provides the backend and </span><span class="c13">Streamlit</span><span class="c2">&nbsp;provides the frontend. Docker desktop app and Docker Hub is used to manage the images that are pushed to AWS to an EC2 instance which serves the ML model as microservices. </span></p><p class="c1"><span class="c2">First, documentation and data files of the survey were downloaded from the Australian Data Archive. The next step was to explore the data, read through the survey, and select questions that might be relevant to predicting the risk of a heart attack. Then I visualized the different features and looked for correlations. After that, I needed to clean and prepare the data for the machine learning models. I imputed missing data, renamed attributes, and scaled continuous features. Next, I trained several ML models and compared their performance metrics. The models I compared are: Logistic Regression, SupportVectorClassifier, Decision Tree Classifier, Random Forest Classifier, KNN Classifier, and ANN with tensorflow. After analyzing the results, I chose the best performing model and built a web application with microservices. The final step was to deploy the web application using a cloud service, AWS. </span></p><h1 class="c18" id="h.29is1eftsr6w"><span class="c40">Data description</span></h1><p class="c1"><span class="c2">The dataset &ldquo;Risk Factor Prevalence Study, 1980&rdquo; was obtained from the Australian Data Archive (ADA). The survey documentation is public, available for anyone to read, but to download the data, one needs permission from the ADA. Citing requirements, as well as a copyright and disclaimer can be found below. </span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13">After requesting and gaining access to the dataset, I could download 7 files: the documentation and data of the survey from the website: </span><span class="c27 c13"><a class="c31" href="https://www.google.com/url?q=https://dataverse.ada.edu.au/dataset.xhtml?persistentId%3Ddoi:10.26193/BYE1RE&amp;sa=D&amp;source=editors&amp;ust=1646873339915998&amp;usg=AOvVaw0dAcLVdRYxm8ul0EKZM4V4">https://dataverse.ada.edu.au/dataset.xhtml?persistentId=doi:10.26193/BYE1RE</a></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">Documentation files:</span></p><p class="c1"><span class="c2">Codebook Risk Factor Prevalence Study, 1980.rtf</span></p><p class="c1"><span class="c2">National Heart Foundation Risk Factor Study.pdf</span></p><p class="c1"><span class="c2">Risk Factor Prevalence Study, 1980 - Codebook.pdf</span></p><p class="c1"><span class="c2">Risk Factor Prevalence Study, 1980 - Coding Frame.pdf</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">The data type is a survey; clinical data which was collected from Australian residents in capital cities between the ages 25&ndash;64 in 1980.</span></p><p class="c1"><span class="c2">The primary investigator was the above-mentioned Australia, N. H. F. O., and the survey was partially funded by the Commonwealth Department of Health.</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">Citation Requirement: &ldquo;Australia, N. H. F. O. Risk Factor Prevalence Study, 1980 [computer file]. Canberra: Australian Data Archive, The Australian National University&rdquo;.</span></p><p class="c1"><span class="c2">Rights &amp; Disclaimer:</span></p><p class="c1"><span class="c2">&ldquo;Use of the material is solely at the user&#39;s risk. The depositor, The Australian National University and the Australian Data Archive shall not be held responsible for the accuracy and completeness of the material supplied.&rdquo;</span></p><p class="c1"><span class="c2">Copyright: </span></p><p class="c1"><span class="c2">Copyright &copy; 2005, Depositor: and Contact: Sophia.Ljaskevic@heartfoundation.com.au. All rights reserved.</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">The dataset has 169 variables with 5,603 cases, the sas7bdat data file is 7.7 MB. The variables of the dataset follow the questions of the survey. From the 169 variables, I used 34 from the original dataset to predict a heart attack. I added a calculated variable: BMI, since it is a better indicator of obesity than weight and height separately. I used the formula:</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c35"><span class="c13">BMI = weight (kg) / (height (m))</span><span class="c13 c50">2</span><span class="c2">&nbsp;</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13">taken from the CDC website: </span><span class="c13 c27"><a class="c31" href="https://www.google.com/url?q=https://www.cdc.gov/nccdphp/dnpao/growthcharts/training/bmiage/page5_1.html&amp;sa=D&amp;source=editors&amp;ust=1646873339918742&amp;usg=AOvVaw2pkAZYORJSb7PQxDzD5oqD">https://www.cdc.gov/nccdphp/dnpao/growthcharts/training/bmiage/page5_1.html</a></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13">I renamed the variables from the question numbers to more </span><span class="c13">specified</span><span class="c13">&nbsp;names. Variable 8, &ldquo;</span><span class="c3 c13">heart_attack&rdquo;</span><span class="c2">, from the list below is the target data, and all remaining variables are the feature data. </span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">Variables with new names:</span></p><p class="c1"><span class="c2 c3"># &nbsp; Column &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Non-Null Count &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dtype &nbsp;</span></p><p class="c1"><span class="c2 c3">--- &nbsp;------ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-------------- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;----- &nbsp;</span></p><p class="c1"><span class="c2 c3">&nbsp;0 &nbsp; sex &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;1 &nbsp; age &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;2 &nbsp; BMI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;3 &nbsp; chestpain &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5601 non-null &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;4 &nbsp; chestpressure &nbsp; &nbsp;5595 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;5 &nbsp; diabetes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;6 &nbsp; HBP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;7 &nbsp; angina &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;8 &nbsp; heart_attack &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;9 &nbsp; stroke &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;10 &nbsp;highcholesterol 5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;11 &nbsp;hightriglyc &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;12 &nbsp;sleep_aid &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5602 non-null &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;13 &nbsp;sedatives &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5600 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;14 &nbsp;vitamins &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5600 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;15 &nbsp;sleep_hours &nbsp; &nbsp; &nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;16 &nbsp;sleep_quality &nbsp; &nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;17 &nbsp;add_salt &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;18 &nbsp;meat &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;19 &nbsp;fat &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;20 &nbsp;eggs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5543 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;21 &nbsp;fat_type &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;22 &nbsp;alcohol &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;23 &nbsp;num_drinks &nbsp; &nbsp; &nbsp; 5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;24 &nbsp;walking &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;25 &nbsp;cardio &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;26 &nbsp;sport &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5598 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;27 &nbsp;hobby &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5599 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;28 &nbsp;work &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3907 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;29 &nbsp;SYS1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;30 &nbsp;SYS2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;31 &nbsp;DIA1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;32 &nbsp;DIA2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5603 non-null &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; float64</span></p><p class="c1"><span class="c2 c3">&nbsp;33 &nbsp;SERCHOL &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c3 c13">&nbsp;34 &nbsp;</span><span class="c3 c13">HDLCHOL</span><span class="c2 c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5603 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">&nbsp;35 &nbsp;TRIGLYC &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;5574 non-null &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float64</span></p><p class="c1"><span class="c2 c3">dtypes: float64(36)</span></p><p class="c1"><span class="c2 c3">memory usage: 1.5 MB</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c42 c4">Variables description:</span></p><p class="c1 c5"><span class="c11"></span></p><a id="t.eda448faee49c1eb075548b31616280401253324"></a><a id="t.0"></a><table class="c32"><tbody><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c10 c5"><span class="c11"></span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c11">Variable name</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c11">Description</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c0"><span class="c11">Values</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c0"><span class="c11">Data type</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">0</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2">sex</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2">sex of the patient</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: male</span></p><p class="c10"><span class="c2">2: female</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">1</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2">age</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2">age of the patient</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: 24 &ndash; 29</span></p><p class="c10"><span class="c2">2: 30 &ndash; 34</span></p><p class="c10"><span class="c2">3: 35 &ndash; 39</span></p><p class="c10"><span class="c2">4: 40 &ndash; 44</span></p><p class="c10"><span class="c2">5: 45 &ndash; 49</span></p><p class="c10"><span class="c2">6: 50 &ndash; 54</span></p><p class="c10"><span class="c2">7: 55 &ndash; 59</span></p><p class="c10"><span class="c2">8: 60 &ndash; 64</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">2</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2">BMI</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2">calculated from weight and height</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">numeric</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">continuous</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">3</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2">chestpain</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2">responder experienced chest pain</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: no</span></p><p class="c10"><span class="c2">2: yes</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">4</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">chestpressure</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c13">esponder experienced </span><span class="c3 c13">chest pressure</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: no</span></p><p class="c10"><span class="c2">2: yes</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">5</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">diabetes</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">responder has diabetes</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: no</span></p><p class="c10"><span class="c2">2: yes</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">6</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">HBP</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">responder has HBP</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: no</span></p><p class="c10"><span class="c2">2: yes</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">7</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2">angina</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">responder has </span><span class="c2">angina pectoris</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: no</span></p><p class="c10"><span class="c2">2: yes</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">8</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">heart_attack</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">responder had heart_attack</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: no</span></p><p class="c10"><span class="c2">2: yes</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">9</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2">stroke</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">responder had </span><span class="c2">stroke</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: no</span></p><p class="c10"><span class="c2">2: yes</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">10</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">highcholesterol</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">responder has high cholesterol</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: no</span></p><p class="c10"><span class="c2">2: yes</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">11</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">hightriglyc</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">responder has high triglyceride levels</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2">1: no</span></p><p class="c10"><span class="c2">2: yes</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">12</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">sleep_aid</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">frequency in which the responder takes sleeping aids</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: every day</span></p><p class="c10"><span class="c2 c3">2: a few days a week</span></p><p class="c10"><span class="c2 c3">3: once a week</span></p><p class="c10"><span class="c2 c3">4: occasionally</span></p><p class="c10"><span class="c2 c3">5: rarely</span></p><p class="c10"><span class="c2 c3">6: never</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">13</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">sedatives</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">frequency in which the responder takes sedatives or tranquilizers</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: every day</span></p><p class="c10"><span class="c2 c3">2: a few days a week</span></p><p class="c10"><span class="c2 c3">3: once a week</span></p><p class="c10"><span class="c2 c3">4: occasionally</span></p><p class="c10"><span class="c2 c3">5: rarely</span></p><p class="c10"><span class="c2 c3">6: never</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">14</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">vitamins</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how often the responder takes vitamins or mineral supplements</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: every day</span></p><p class="c10"><span class="c2 c3">2: a few days a week</span></p><p class="c10"><span class="c2 c3">3: once a week</span></p><p class="c10"><span class="c2 c3">4: occasionally</span></p><p class="c10"><span class="c2 c3">5: rarely</span></p><p class="c10"><span class="c2 c3">6: never</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">15</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">sleep_hours</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">the number of hours the responder usually sleeps in a night</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: 5 hours or less</span></p><p class="c10"><span class="c2 c3">2: 6 hours</span></p><p class="c10"><span class="c2 c3">3: 7 hours</span></p><p class="c10"><span class="c2 c3">4: 8 hours</span></p><p class="c10"><span class="c2 c3">5: 9 hours or more</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">16</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">sleep_quality</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how the responder describes their normal sleep</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: poor</span></p><p class="c10"><span class="c2 c3">2: fair</span></p><p class="c10"><span class="c2 c3">3: good</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">17</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">add_salt</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how often the responder adds salt to food</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: not at all</span></p><p class="c10"><span class="c2 c3">2: sometimes</span></p><p class="c10"><span class="c2 c3">3: only after fasting</span></p><p class="c10"><span class="c2 c3">4: always</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">18</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">meat</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how often the responder eats meat</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: every day</span></p><p class="c10"><span class="c2 c3">2: most days</span></p><p class="c10"><span class="c2 c3">3: at least once a week</span></p><p class="c10"><span class="c2 c3">4: infrequently</span></p><p class="c10"><span class="c2 c3">5: never</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">19</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">fat</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how often the responder eats the fat on meat</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: every day</span></p><p class="c10"><span class="c2 c3">2: most days</span></p><p class="c10"><span class="c2 c3">3: at least once a week</span></p><p class="c10"><span class="c2 c3">4: infrequently</span></p><p class="c10"><span class="c2 c3">5: never</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">20</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">eggs</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how many eggs the responder eats per week</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">numeric</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">numeric</span></p></td></tr><tr class="c38"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">21</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">fat_type</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">which type of fat the responder eats most often</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: butter</span></p><p class="c10"><span class="c2 c3">2: polyunsaturated margarine</span></p><p class="c10"><span class="c2 c3">3: other table margarines</span></p><p class="c10"><span class="c2 c3">4: I rarely eat any of these</span></p><p class="c10"><span class="c2 c3">5: I don&rsquo;t eat any of these</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c46"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">22</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">alcohol</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how often the responder drinks alcohol</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: don&rsquo;t drink alcohol</span></p><p class="c10"><span class="c2 c3">2: less than once a week</span></p><p class="c10"><span class="c2 c3">3: on 1 or 2 days a week</span></p><p class="c10"><span class="c2 c3">4: on 3 or 4 days a week</span></p><p class="c10"><span class="c2 c3">5: on 5 or 6 days a week</span></p><p class="c10"><span class="c2 c3">6: every day</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c43"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">23</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">num_drinks</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">Number of drinks the responder consumes per occasion</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: don&rsquo;t drink alcohol</span></p><p class="c10"><span class="c2 c3">2: 1 or 2 drinks</span></p><p class="c10"><span class="c2 c3">3: 3 or 4 drinks</span></p><p class="c10"><span class="c2 c3">4: 5 to 8 drinks</span></p><p class="c10"><span class="c2 c3">5: 9 to 12 drinks</span></p><p class="c10"><span class="c2 c3">6: 13 to 20 drinks</span></p><p class="c10"><span class="c2 c3">7: more than 20 drinks</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c47"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">24</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">walking</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how often the responder walks for exercise</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: three times or more a week</span></p><p class="c10"><span class="c2 c3">2: once or twice a week</span></p><p class="c10"><span class="c2 c3">3: once a month</span></p><p class="c10"><span class="c2 c3">4: rarely</span></p><p class="c10"><span class="c2 c3">5: never</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c48"><td class="c17" colspan="1" rowspan="1"><p class="c10"><span class="c2">25</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">cardio</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how often the responder exercises vigorously</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: three times or more a week</span></p><p class="c10"><span class="c2 c3">2: once or twice a week</span></p><p class="c10"><span class="c2 c3">3: once a month</span></p><p class="c10"><span class="c2 c3">4: rarely</span></p><p class="c10"><span class="c2 c3">5: never</span></p><p class="c10 c5"><span class="c2 c3"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">26</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">sport</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how often the responder engages in other sports</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: three times or more a week</span></p><p class="c10"><span class="c2 c3">2: once or twice a week</span></p><p class="c10"><span class="c2 c3">3: once a month</span></p><p class="c10"><span class="c2 c3">4: rarely</span></p><p class="c10"><span class="c2 c3">5: never</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">27</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">hobby</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">other physical activities, like hobbies</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: three times or more a week</span></p><p class="c10"><span class="c2 c3">2: once or twice a week</span></p><p class="c10"><span class="c2 c3">3: once a month</span></p><p class="c10"><span class="c2 c3">4: rarely</span></p><p class="c10"><span class="c2 c3">5: never</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">28</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">work</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">how much time the responder spends walking while working</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">1: practically all</span></p><p class="c10"><span class="c2 c3">2: more than half</span></p><p class="c10"><span class="c2 c3">3: about half</span></p><p class="c10"><span class="c2 c3">4: less than half</span></p><p class="c10"><span class="c2 c3">5: almost none</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">discrete</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">29</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">SYS1</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">systolic blood pressure first measure</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">numeric</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">continuous</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">30</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">SYS2</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">systolic blood pressure second measure</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">numeric</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">continuous</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">31</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">DIA1</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">diastolic blood pressure first measure</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">numeric</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">continuous</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">32</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">DIA2</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">diastolic blood pressure second measure</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">numeric</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">continuous</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">33</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">SERCHOL</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">serum cholesterol level</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">numeric</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">continuous</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">34</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c3 c13">HDLCHOL</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">HDL cholesterol level</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">numeric</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">continuous</span></p></td></tr><tr class="c15"><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c2">35</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">TRIGLYC</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">triglyceride levels</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c10"><span class="c2 c3">numeric</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c10"><span class="c2">continuous</span></p><p class="c10 c5"><span class="c2"></span></p></td></tr></tbody></table><h1 class="c18 c39" id="h.97yhjjke9llx"><span class="c40"></span></h1><h1 class="c18" id="h.jrpc7smvf66f"><span class="c40">Software</span></h1><p class="c1"><span class="c2">I used the Jupyter Notebook with Python 3.7 for the model building part, and FastAPI and Streamlit for the web application. Docker was used to build images for the web server which is an EC2 instance on AWS. Zoom will be used for the video presentation. </span></p><p class="c1"><span class="c2">Python libraries: numpy, pandas, math, matplotlib, pyplot, seaborn, pyreadstat, sklearn, scipy, tensorflow, pickle, uvicorn, fastapi, streamlit.</span></p><h1 class="c18 c39" id="h.sssrojb6pce2"><span class="c40"></span></h1><h1 class="c18" id="h.oqtq2kn72csi"><span class="c40">Analyses</span></h1><h2 class="c37" id="h.5yglaw81yp4o"><span class="c42 c51">Analysis description</span></h2><p class="c1"><span class="c2">First, I got familiar with the dataset. I thoroughly read the survey documentation and explored the variables of the dataset. After choosing the variables I planned to work with, I started the data exploration with descriptive statistics and data visualization. Graphs helped to find and understand relationships between the variables. We also checked the distribution and skewness of the data. Boxplots are useful for detecting outliers, histograms present distribution, and scatter plots show relationships between two continuous variables.</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">Next I cleaned and prepared the data for the ML algorithms. I addressed the missing values with imputation. Because the dataset is relatively small, I didn&rsquo;t drop any missing values, but for both categorical and continuous features, I used KNN Imputer. For the categorical features, missing values were &ldquo;nan&rdquo;, while for the continuous data it was zero. Therefore, I replaced zeros with &ldquo;nan&rdquo; for easy imputation. The categorical features first were ordinal encoded then imputed and finally I got the labels back by reversing ordinal encoding. After the continuous features were imputed too, I joined all features back together into one dataframe. After I had no missing values, I continued data exploration with the info(), describe(), and corr() methods. Since I wanted to predict heart disease, I compared patients&#39; data with and without heart attack using boxplots, barplots, scatterplots and cat plots with the seaborn library. Finally, I scaled the continuous data using the Standard Scaler from scikit-learn preprocessing library.</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">After I had a clean and preprocessed dataset, I was ready to move on to model building. I created two dataframes: features and target. The target data frame contains only one column, the feature I want to predict: &ldquo;heart_attack&rdquo;. The features dataframe has the remaining 33 features as described above in the Data Description part.</span></p><p class="c1"><span class="c2">Then, I split the data into training data and test data using train_test_split and 20% test set size. I used the test set after the hyperparameters were fine-tuned.</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c4">ML Model Training:</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">Logistic Regression: with penalty, C, solver</span></p><p class="c1"><span class="c2">Support Vector Classification: C, kernel, degree, gamma, coef0</span></p><p class="c1"><span class="c13">Decision Tree Classifier: </span><span class="c29 c3 c13">criterion(default), splitter(default)</span><span class="c2">, max_depth, min_samples_split, min_samples_leaf (influences only the feature importance list), max_features</span></p><p class="c1"><span class="c13">Random Forest Classifier: </span><span class="c29 c3 c13">criterion, </span><span class="c2">n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes</span></p><p class="c1"><span class="c2">KNN: n_neighbors, weights, algorithm, leaf_size, metric - default worked best</span></p><p class="c1"><span class="c2">ANN with keras and tensorflow. </span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13 c16">Logistic Regression</span><span class="c2">: penalty, C, solver parameter were used. </span></p><p class="c1"><span class="c2 c3">The logistic regression model is a binary classifier, uses the S-shaped logistic function: estimates the probability of the positive class. If the probability is greater than 0.5, the instance belongs to the positive class, if it is less, then the model predicts that it belongs to the negative class. To calculate the probability, the model computes the weighted sum of the input features (plus a bias term), and it outputs the logistic of this result. Since this value is always between 0 and 1, the model then easily computes the probability.</span></p><p class="c1"><span class="c2">First, I trained a Logistic Regression model with default values and checked the accuracy, precision and recall scores. These scores were the same, and relatively high. We also used the trained model on the training set to check if the model is overfitted. Since the metrics came back very close to the test set scores, I can conclude that the model was not overfitted. I plotted a confusion matrix to check false positives and false negatives in the prediction. The model predicted 11 positive cases for heart attack, but only 4 were true positives. 7 were false positives, and 13 were false negatives.</span></p><p class="c1"><span class="c13">After that I performed a grid search to find the best parameters for penalty, C, solver, starting with broad intervals then narrowing them in the following grid searches. After 2 grid searches I found the best parameters: </span><span class="c2 c3">&#39;C=206.913808111479, penalty= l2, solver= liblinear. We trained a new model with the best parameters and checked the generalization error with the same metrics and MSE. After grid search, the confusion matrix was the same. </span></p><p class="c1"><span class="c2 c3">I also checked the feature importances to understand the model better. The top six features in predicting a heart attack are: angina, chest pain, stroke, chest pressure, high triglyceride levels and high cholesterol.</span></p><p class="c1"><span class="c2 c3">Best parameters:</span></p><p class="c1"><span class="c2 c3">C = 207, penalty = &#39;l2&#39;, solver = &#39;liblinear&#39;</span></p><p class="c1 c5"><span class="c2 c3"></span></p><p class="c1"><span class="c13 c16">Support Vector Classification</span><span class="c2">: with C, kernel, degree, gamma, coef0</span></p><p class="c1"><span class="c3 c13">Th</span><span class="c3 c13">e second model was a vector classification model. </span><span class="c3 c13">The </span><span class="c13">Support Vector Classifier is a linear model, with an </span><span class="c3 c13">algorithm that creates a line or a hyperplane to separate the data into classes. Since the gamma value is low in the best model, that means that even the data points far away from the boundary get considerable weight and the curve is more linear.</span></p><p class="c1"><span class="c3 c13">After the grid search, I trained a model with the best parameters, and checked the performance metrics. Accuracy, precision, and recall scores came back the same, 98.48 and MSE=0.015. The model predicted 8 positive cases for heart attack, 4 true</span><span class="c2 c3">&nbsp;positives and 4 false positives. There were also 13 false negative cases. </span></p><p class="c1"><span class="c2 c3">Then I performed dimensionality reduction with PCA to see how the results change while preserving 95% variance. The performance metrics were the same after the PCA, but the confusion matrix was slightly different. Now the model predicted zero positive cases, and 17 false negatives. </span></p><p class="c1"><span class="c2 c3">Best parameters:</span></p><p class="c1"><span class="c2 c3">C=5</span></p><p class="c1"><span class="c2 c3">coef0=1</span></p><p class="c1"><span class="c2 c3">gamma=0.005</span></p><p class="c1"><span class="c2 c3">kernel=&#39;poly&#39;</span></p><p class="c1 c5"><span class="c2 c3"></span></p><p class="c1 c5"><span class="c11"></span></p><p class="c1"><span class="c13 c16">Decision Tree Classifie</span><span class="c2">r: </span></p><p class="c1"><span class="c2 c3">The Decision Tree algorithm splits the nodes to have the lowest gini scores. If the model is not constrained, it splits nodes until it reaches the lowest gini scores. Then it stops splitting further and the node becomes a leaf. In practice, the gini score is calculated for every node, then the weighted average of these scores becomes the gini of the given feature/question. Then after every features&rsquo; gini score is calculated this way, the feature with the lowest gini score will be the next feature that splits the dataset.</span></p><p class="c1"><span class="c13">The decision tree classifier model was trained with </span><span class="c3 c13 c29">criterion(default), splitter(default)</span><span class="c13">, max_depth, min_samples_split, min_samples_leaf (influences only the feature importance list), and max_features. However, I first checked how a default model performs: the metrics were slightly lower than with the previous models: accuracy, precision, and recall scores = </span><span class="c2 c3">0.97056. The model predicted 22 positive cases, from which 19 were false positives. There were also 14 false negative cases. The feature importance list was a little different from the logistic regression model: the top six features were angina, BMI, systolic blood pressure, walking time during work, number of alcoholic drinks per day, and sleep hours per night. </span></p><p class="c1"><span class="c2 c3">After performing grid search, I trained a new model with the best parameters and checked the performance metrics: they slightly improved: accuracy score, precision, and recall scores = 0.984 and MSE=0.016. From the confusion matrix I learned that the model predicted 3 positive cases from which 2 were false positives. And there were also 16 false negatives. After further tuning the model by hand, I found another model which performed slightly better: &nbsp;accuracy, precision, and recall scores were 0.9866. The model predicted 8 positive cases, but 3 were false positives. There were also 12 false negative cases. The feature importances also changed.</span></p><p class="c1"><span class="c2 c3">The best parameters were:</span></p><p class="c1"><span class="c2 c3">max_depth=8</span></p><p class="c1"><span class="c2 c3">max_leaf_nodes=30</span></p><p class="c1"><span class="c2 c3">min_samples_split=20 </span></p><p class="c1"><span class="c2 c3">max_features=12</span></p><p class="c1"><span class="c2 c3">min_samples_leaf=12</span></p><p class="c1"><span class="c3 c13 c21">criterion(default)=gini</span></p><p class="c1"><span class="c29 c3 c13">splitter(default)=best</span></p><p class="c1 c5"><span class="c2 c3"></span></p><p class="c1"><span class="c13 c16">Random Forest Classifier</span><span class="c13">: with </span><span class="c29 c3 c13">criterion, </span><span class="c2">n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes</span></p><p class="c1"><span class="c3 c13">The random forest model is a tree based classifier that splits nodes to minimize the gini score or entropy. While one single decision tree trains fast, a random forest takes much longer as it grows many random trees.</span></p><p class="c1"><span class="c2 c3">The first model with default parameters predicted 2 true positive cases and 15 false negatives. The metrics were (accuracy, precision, recall scores): 0.9866.</span></p><p class="c1"><span class="c2 c3">After grid search, the scores slightly decreased to 0.9848 and the MSE=0.151.</span></p><p class="c1"><span class="c2 c3">The feature importances was similar to the best decision tree model. </span></p><p class="c1"><span class="c2 c3">The best parameters are:</span></p><p class="c1"><span class="c2 c3">max_depth=32</span></p><p class="c1"><span class="c2 c3">max_leaf_nodes=18</span></p><p class="c1"><span class="c2 c3">min_samples_leaf=2</span></p><p class="c1"><span class="c2 c3">n_estimators=100</span></p><p class="c1"><span class="c2 c3">criterion=&#39;gini&#39;</span></p><p class="c1 c5"><span class="c2 c3"></span></p><p class="c1"><span class="c13 c16">KNN Classifier</span><span class="c13">: For this algorithm, n_neighbors, weights, algorithm, leaf_size, metric parameters were used, and the default parameters gave the same result as the best parameters. </span><span class="c3 c13">The KNN algorithm classifies a new datapoint by comparing it to a given number of its nearest neighbors. </span><span class="c13">The models predicted 2 positive cases from which 1 was false positive. There were 16 false negatives as well. The </span><span class="c2 c3">accuracy, precision, recall scores were 0.9848. The best parameters were:</span></p><p class="c1"><span class="c2">n_neighbors=5</span></p><p class="c1"><span class="c2 c3">algorithm= &#39;auto&#39;</span></p><p class="c1"><span class="c2 c3">leaf_size=10</span></p><p class="c1"><span class="c2 c3">metric=&#39;minkowski&#39;</span></p><p class="c1"><span class="c2 c3">weights= &#39;uniform&#39;</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13 c16">ANN with keras and tensorflow</span><span class="c2">: </span></p><p class="c1"><span class="c13">For the ANN model, the training data was further divided into training and validation sets. 450 data points were used for cross validation. After fine tuning the parameters, the following model performed the best: I trained a model with four dense layers, 1 input layer, 2 hidden layers and 1 output layer, with &lsquo;relu&rsquo; and &lsquo;softmax&rsquo; activations. The layers have 5, 3, 3, and 1 neurons. We used 400 epochs with early stopping and with patience=10. </span><span class="c3 c13">The MSE is computed on the validation samples. I also used learning curve graphs to check if the validation loss got close to the training loss. </span><span class="c13">The accuracy score was 0.9848 and MSE=0.0152.</span></p><p class="c1 c5"><span class="c11"></span></p><p class="c1 c5"><span class="c11"></span></p><p class="c1"><span class="c4">ML Model Results and Discussion</span><span class="c26">:</span></p><p class="c1"><span class="c2">The hyperparameters were fine-tuned using grid search to find the best parameters. Finding the best parameters to constrain the models helps prevent overfitting. Our goal when training ML models is not to get the best performance metric scores, but to have a model that generalizes well on new, unseen data. I also added some visuals to help understand the results: tree plots and confusion matrices. I compared the performance metrics (accuracy, precision, recall, MSE) which is summarized in the table below:</span></p><p class="c1 c5"><span class="c11"></span></p><a id="t.c29afc18296717b9dfe4a3f614704c8a2943c01d"></a><a id="t.1"></a><table class="c32"><tbody><tr class="c15"><td class="c41" colspan="1" rowspan="1"><p class="c0"><span class="c11">Models</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c11">Accuracy</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c11">Precision</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c11">Recall</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c0"><span class="c11">MSE</span></p></td></tr><tr class="c15"><td class="c41" colspan="1" rowspan="1"><p class="c10"><span class="c11">Logistic Regression</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9822</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9822</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9822</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.0152</span></p></td></tr><tr class="c15"><td class="c41" colspan="1" rowspan="1"><p class="c10"><span class="c11">SVM Classifier</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.0152</span></p></td></tr><tr class="c15"><td class="c41" colspan="1" rowspan="1"><p class="c10"><span class="c11">Decision Tree Classifier</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9839</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9839</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9839</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.0161</span></p></td></tr><tr class="c15"><td class="c41" colspan="1" rowspan="1"><p class="c10"><span class="c11">Random Forest Classifier</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.0152</span></p></td></tr><tr class="c15"><td class="c41" colspan="1" rowspan="1"><p class="c10"><span class="c11">KNN Classifier</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.0152</span></p></td></tr><tr class="c15"><td class="c41" colspan="1" rowspan="1"><p class="c10"><span class="c11">ANN</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.9848</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0 c5"><span class="c2"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c0 c5"><span class="c2"></span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c0"><span class="c2">0.0152</span></p></td></tr></tbody></table><p class="c1 c5"><span class="c11"></span></p><p class="c1"><span class="c2">As we can see, the performance metric scores are identical or very similar in every model. However, the confusion matrices are different and offer additional insight: I chose the SVM Classifier model for the web application since it predicted the least numbers of false negatives and false positives. I think when we try to predict a deadly disease, it is important to keep the false negative numbers to a minimum. In this way, we won&rsquo;t miss those patients who might be in danger of having a heart attack. </span></p><p class="c1"><span class="c2">While the scores were very similar, some of the models did not predict any positive cases. I think this is due to the fact that the dataset contained only 108 positive cases in the 5603 data points. Considering this relatively low number, I think most models had difficulty learning to predict positive cases. We could probably improve this result by combining several years of data into one dataset, and dropping some of the negative cases so the ratio of positive and negative cases could improve. </span></p><p class="c1 c5"><span class="c2"></span></p><a id="t.fe080c067bc08ac611dda5222a6bf6735646273e"></a><a id="t.2"></a><table class="c32"><tbody><tr class="c15"><td class="c20" colspan="1" rowspan="1"><p class="c0"><span class="c11">Models</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c11">True Positive</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c0"><span class="c11">True Negative</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c11">False Positive</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c0"><span class="c11">False Negative</span></p></td></tr><tr class="c15"><td class="c20" colspan="1" rowspan="1"><p class="c10"><span class="c11">Logistic Regression</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c2">4</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c0"><span class="c2">1097</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c2">7</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c0"><span class="c2">13</span></p></td></tr><tr class="c15"><td class="c20" colspan="1" rowspan="1"><p class="c10"><span class="c11">SVM Classifier</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c2">4</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c0"><span class="c2">1100</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c2">4</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c0"><span class="c2">13</span></p></td></tr><tr class="c15"><td class="c20" colspan="1" rowspan="1"><p class="c10"><span class="c11">Decision Tree Classifier</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c2">1</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c0"><span class="c2">1102</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c2">2</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c0"><span class="c2">16</span></p></td></tr><tr class="c15"><td class="c20" colspan="1" rowspan="1"><p class="c10"><span class="c11">Random Forest Classifier</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c2">0</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c0"><span class="c2">1104</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c2">0</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c0"><span class="c2">17</span></p></td></tr><tr class="c15"><td class="c20" colspan="1" rowspan="1"><p class="c10"><span class="c11">KNN Classifier</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c2">1</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c0"><span class="c2">1103</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c2">1</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c0"><span class="c2">16</span></p></td></tr><tr class="c15"><td class="c20" colspan="1" rowspan="1"><p class="c10"><span class="c11">ANN</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c2">1</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c0"><span class="c2">1103</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c2">1</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c0"><span class="c2">16</span></p></td></tr></tbody></table><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">After I picked the best model for the web application, I used the pickle library to save the model for FastAPI.</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1 c5"><span class="c4 c42"></span></p><p class="c1"><span class="c4">ML Model User-Interface:</span></p><p class="c1"><span class="c2">The final step was to build a web application using FastAPI and Streamlit, and to launch it on AWS.</span></p><p class="c1"><span class="c13">I planned to deploy the machine learning model in Docker containers and </span><span class="c13">serv</span><span class="c2">e them as microservices. The advantage of microservices is that the project can be divided into smaller components and they can be deployed independently. This gives us more flexibility to maintain the software: every microservice can be updated and optimized separately which in the end saves resources. In order to use the microservice architecture, we need containerization. I used Docker containers to package the frontend and the backend separately with all their dependencies and isolated environments. Dockerfiles are used to define the containers and build Docker images. Then the images are used on a server as blueprints for the containers. The containers communicate via an API: the frontend container sends the requests - inputted by the user - and the backend container which contains the trained model sends predictions back to the frontend. We also need Docker-compose, which is a tool that makes it possible for the two containers to communicate with each other.</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13">First, I built the </span><span class="c13">backend</span><span class="c13">&nbsp;of the application with FastAPI. We need to import the needed libraries unicorn, pickle, sys, FastAPI, and </span><span class="c13">pydantic</span><span class="c13">. Then we use the BaseModel from </span><span class="c13">pydantic</span><span class="c2">&nbsp;to declare the variables and their data type. Next, I created an object from the FastAPI class. Then I to built two endpoints: one for the input data and one for the prediction. Then input data from the frontend was processed and forwarded to the model for prediction. The code for the backend is found in the &ldquo;api.py&rdquo; Python file. We can use uvicorn to serve the backend by the command: &ldquo;uvicorn api:api&rdquo; in the terminal. FastAPI has an automatically generated documentation which is also useful for initial testing of the model. </span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13">For building the </span><span class="c13">frontend</span><span class="c13">, I used Streamlit which provides a simple but user friendly interface. First, we import the necessary libraries: streamlit, pandas, requests, json, and </span><span class="c13">pydantic</span><span class="c13">. We use the BaseModel from </span><span class="c13">pydantic</span><span class="c2">&nbsp;to declare the variables and their data type. Next, we get input data from the streamlit frontend. Then the input data is combined into a JSON object and a post request is made to the &ldquo;prediction&rdquo; endpoint. There is also a &ldquo;Show Details&#39;&#39; checkbox that provides some explanations on how the app works. The code for the frontend is in the app.py. To run the frontend, we use the command: &ldquo;streamlit run app.py.</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c13">Now that both the frontend and backend have been built, we can test our application locally by running them from the terminal: </span><span class="c27 c13"><a class="c31" href="https://www.google.com/url?q=http://localhost:8501&amp;sa=D&amp;source=editors&amp;ust=1646873340043853&amp;usg=AOvVaw2MJ74NkGQaxiIPebLxWRon">http://localhost:8501</a></span><span class="c2">.</span></p><p class="c1"><span class="c13">The next step is </span><span class="c13">containerizing</span><span class="c2">&nbsp;the backend and frontend separately with Docker. We need a different Dockerfile for each container. The Dockerfile is a text file with the commands to build a Docker image. We also need to include the dependencies for each container. The dependencies are in the &ldquo;requirements.txt&rdquo; file which was automatically generated by the &ldquo;pipreqs&rdquo; package. In the Dockerfile we can define the work directory for the application and copy all the files to the container. I built the images using the &ldquo;docker build -t &lt;frontend_or_backend_name&gt; .&rdquo;. I downloaded the Docker desktop app which makes it easy to manage images and containers. I also set up an account in Docker Hub to create a repository and then pushed the images and docker-compose.yml there.</span></p><p class="c1"><span class="c2">Next step was to create a docker-compose.yml file that coordinates the communication between the frontend and backend containers. In this file we define the services, the working directory, the container and image names and the ports that will be used. </span></p><p class="c1"><span class="c2">I set up the following file structure:</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 275.76px; height: 284.50px;"><img alt="" src="images/image1.png" style="width: 275.76px; height: 284.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c2">We run the docker-compose.yml file from the directory which is located in with the command: &ldquo;docker-compose up&rdquo;.</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c2">To be able to deploy the application on AWS, first I needed to create an account and set up an EC2 instance which is a virtual machine provided by Amazon. The instance is based on the Linux 2 AMI which is a t2.micro instance type. The instance is in the US-East 1 region and I chose the us-east 1a availability zone. After creating the instance, I could choose to get a new private key which Amazon generated.</span></p><p class="c1"><span class="c2">&nbsp;</span></p><p class="c1"><span class="c2">Then I created a new VPC (virtual private cloud) and chose a private IP block: 10.0.0.0 and then Amazon picked the private IP address 10.0.0.245 for the instance. Amazon calls the public IPs elastic IP, which means it is static. Next I allocated an elastic IP address and associated it with the instance. This IP address is region specific. Amazon also generated a default DNS name that resolves to this public IP. In the security section, I added two new security rules to open ports 8000 and 8501 for the backend and frontend.</span></p><p class="c1"><span class="c2">After I set up the instance, I could connect to it with &ldquo;ssh - &lt;URL with path&gt;&rdquo;. Next I installed Docker and docker-compose from GitHub. After that, I pulled the images from the Docker Hub repository using the commands:</span></p><p class="c1"><span class="c2">&quot;docker pull hpiringer/capstone:fastapibackend&quot; and</span></p><p class="c1"><span class="c13">&quot;docker pull hpiringer/capstone:</span><span class="c13">streamlitfrontend</span><span class="c2">&quot;. Then with the &ldquo;docker-compose up&rdquo; command I started the application which is now available on the following URL:</span></p><p class="c1 c5"><span class="c2"></span></p><p class="c1"><span class="c27 c13"><a class="c31" href="https://www.google.com/url?q=http://ec2-52-54-129-72.compute-1.amazonaws.com:8501/&amp;sa=D&amp;source=editors&amp;ust=1646873340045530&amp;usg=AOvVaw0UpydSBcmLwWz_dOIqg4YR">http://ec2-52-54-129-72.compute-1.amazonaws.com:8501/</a></span></p><p class="c1 c5"><span class="c2"></span></p></body></html>