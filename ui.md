## User Interface

### ML Model User Interface:

The final step was to build a web application using FastAPI and Streamlit, and to launch
it on AWS.

I planned to deploy the machine learning model in Docker containers and serve them as
microservices. The advantage of microservices is that the project can be divided into
smaller components and they can be deployed independently. This gives us more
flexibility to maintain the software: every microservice can be updated and optimized
separately which in the end saves resources. In order to use the microservice
architecture, we need containerization. I used Docker containers to package the
frontend and the backend separately with all their dependencies and isolated
environments. Dockerfiles are used to define the containers and build Docker images.
Then the images are used on a server as blueprints for the containers. The containers
communicate via an API: the frontend container sends the requests - inputted by the
user - and the backend container which contains the trained model sends predictions
back to the frontend. We also need Docker-compose, which is a tool that makes it
possible for the two containers to communicate with each other.

First, I built the backend of the application with FastAPI. We need to import the needed
libraries unicorn, pickle, sys, FastAPI, and pydantic. Then we use the BaseModel from
pydantic to declare the variables and their data type. Next, I created an object from the
FastAPI class. Then I to built two endpoints: one for the input data and one for the
prediction. Then input data from the frontend was processed and forwarded to the
model for prediction. The code for the backend is found in the “api.py” Python file. We
can use uvicorn to serve the backend by the command: “uvicorn api:api” in the terminal.
FastAPI has an automatically generated documentation which is also useful for initial
testing of the model.

For building the frontend, I used Streamlit which provides a simple but user friendly
interface. First, we import the necessary libraries: streamlit, pandas, requests, json, and
pydantic. We use the BaseModel from pydantic to declare the variables and their data
type. Next, we get input data from the streamlit frontend. Then the input data is
combined into a JSON object and a post request is made to the “prediction” endpoint.
There is also a “Show Details'' checkbox that provides some explanations on how the
app works. The code for the frontend is in the app.py. To run the frontend, we use the
command: “streamlit run app.py.

Now that both the frontend and backend have been built, we can test our application
locally by running them from the terminal: http://localhost:8501.

The next step is containerizing the backend and frontend separately with Docker. We
need a different Dockerfile for each container. The Dockerfile is a text file with the
commands to build a Docker image. We also need to include the dependencies for each
container. The dependencies are in the “requirements.txt” file which was automatically
generated by the “pipreqs” package. In the Dockerfile we can define the work directory
for the application and copy all the files to the container. I built the images using the
“docker build -t <frontend_or_backend_name> .”. I downloaded the Docker desktop app
which makes it easy to manage images and containers. I also set up an account in
Docker Hub to create a repository and then pushed the images and
docker-compose.yml there.

Next step was to create a docker-compose.yml file that coordinates the communication
between the frontend and backend containers. In this file we define the services, the
working directory, the container and image names and the ports that will be used.

I set up the following file structure:

![tree](http://piringer.github.io/heartdisease/docs/assets/tree.png)

We run the docker-compose.yml file from the directory which is located in with the
command: “docker-compose up”.

To be able to deploy the application on AWS, first I needed to create an account and set
up an EC2 instance which is a virtual machine provided by Amazon. The instance is
based on the Linux 2 AMI which is a t2.micro instance type. The instance is in the
US-East 1 region and I chose the us-east 1a availability zone. After creating the
instance, I could choose to get a new private key which Amazon generated.

Then I created a new VPC (virtual private cloud) and chose a private IP block: 10.0.0.0
and then Amazon picked the private IP address 10.0.0.245 for the instance. Amazon
calls the public IPs elastic IP, which means it is static. Next I allocated an elastic IP
address and associated it with the instance. This IP address is region specific. Amazon
also generated a default DNS name that resolves to this public IP. In the security
section, I added two new security rules to open ports 8000 and 8501 for the backend
and frontend.

After I set up the instance, I could connect to it with “ssh - <URL with path>”. Next I
installed Docker and docker-compose from GitHub. After that, I pulled the images from
the Docker Hub repository using the commands:

  "docker pull hpiringer/capstone:fastapibackend" and

  "docker pull hpiringer/capstone:streamlitfrontend". 

  Then with the “docker-compose up”
command I started the application which is now available on the following URL:

  http://ec2-52-54-129-72.compute-1.amazonaws.com:8501/

<br/><br/>
  
[Home](http://piringer.github.io/heartdisease/index)

[Introduction](http://piringer.github.io/heartdisease/intro)

[Data Description](http://piringer.github.io/heartdisease/Project.pdf)

[Data Exploration and Preparation](http://piringer.github.io/heartdisease/exploration)

[ML Model Training](http://piringer.github.io/heartdisease/models)

[Results and Discussion](http://piringer.github.io/heartdisease/results)

[User Interface](http://piringer.github.io/heartdisease/ui)

[Jupyter Notebook](https://github.com/piringer/heartdisease/blob/main/austral_nb.ipynb)

[Walkthrough Video](https://www.youtube.com/watch?v=18eQWJJu3tA)

[Web App](http://ec2-52-54-129-72.compute-1.amazonaws.com:8501/)

